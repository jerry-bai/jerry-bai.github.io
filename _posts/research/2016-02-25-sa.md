---
layout: research
title:  "Sentiment Analysis on Big Issues"
excerpt: "Research Assistant"
project: true
tag:
- sentiment analysis
- python
- natural language processing
comments: true
---

## Research Assistant

### Topic

Analyze debates on debate.org in order to make accurate predictions on anyone's stand in a debate based on their comments and information.

### Process

* There are 24 big issue topics on debate.org and 20 comments on each topic (10 pro and 10 con). This makes for 480 comments total. I used half for training and half for testing.

* I used a vocabulary of 5000 words to analyze and learn the words in the comments and create a bag of words model.

* I trained a random forest (100 decision trees) based on the comments and whether it was a pro or con.

* Compared my predictions to the gold data.

### Implementation

To parse the debate website and data: `getDebateData.py`.

{% highlight python %}
import urllib
import re
import pandas as pd

sock = urllib.urlopen("http://www.debate.org/big-issues/")
bigIssuesSrc = sock.read()
sock.close()

bigIssuesSrc = bigIssuesSrc.split("BIG")[1].split("Previous")[0] #only html for recent big issues
debate_topics = re.findall(r'<a class="issue-title" href="(.*)" title=', bigIssuesSrc)
num_of_topics = len(debate_topics)

#get even number of debates
if num_of_topics % 2 != 0:
    debate_topics = debate_topics[0:num_of_topics-1]
    num_of_topics -= 1

#split topics in half for training and testing
train_topics = debate_topics[0:num_of_topics/2]
test_topics = debate_topics[num_of_topics/2:]

'''
Get the train data
'''

print "Getting the training data"

train_urls = list()
for train_topic in train_topics:
    train_urls.append("http://www.debate.org" + train_topic)

train_output_comments = list()
train_output_sentiments = list()
train_output_num_comments = 0
for train_url in train_urls:
    print("...")
    train_sock = urllib.urlopen(train_url)
    train_html_src = train_sock.read()
    train_sock.close()

    train_all_comments = re.findall(r'Comment:<\/strong>(.*)', train_html_src)
    train_num_comments = len(train_all_comments)
    train_output_num_comments += train_num_comments
    for i in xrange(0, train_num_comments):
        train_output_comments.append(train_all_comments[i])
        if i <= train_num_comments/2:
            train_output_sentiments.append(1)
        else:
            train_output_sentiments.append(0)

#remove \r (bugging up our output csv)
j = 0
for train_output_comment in train_output_comments:
    train_output_comments[j] = train_output_comment.strip("\r")
    j += 1

outdict = dict()
outdict.update({"comment":train_output_comments})
outdict.update({"sentiment":train_output_sentiments})

train_output = pd.DataFrame(data=outdict, index=range(train_output_num_comments))
train_output.to_csv("labeledDebateTrainData.csv")

'''
Get the test data
'''

print "Getting the testing data"
test_urls = list()
for test_topic in test_topics:
    test_urls.append("http://www.debate.org" + test_topic)

test_output_comments = list()
#test_output_sentiments = list()
test_output_num_comments = 0
for test_url in test_urls:
    print("...")
    test_sock = urllib.urlopen(test_url)
    test_html_src = test_sock.read()
    test_sock.close()

    test_all_comments = re.findall(r'Comment:<\/strong>(.*)', test_html_src)
    test_num_comments = len(test_all_comments)
    test_output_num_comments += test_num_comments
    for i in xrange(0, test_num_comments):
        test_output_comments.append(test_all_comments[i])
        #if i <= test_num_comments/2:
        #    test_output_sentiments.append(1)
        #else:
        #    test_output_sentiments.append(0)

#remove \r (bugging up our output csv)
k = 0
for test_output_comment in test_output_comments:
    test_output_comments[k] = test_output_comment.strip("\r")
    k += 1

outdict = dict()
outdict.update({"comment":test_output_comments})
#outdict.update({"sentiment":test_output_sentiments})

test_output = pd.DataFrame(data=outdict, index=range(test_output_num_comments))
test_output.to_csv("labeledDebateTestData.csv")

'''
Get gold test data
'''

print "Getting the gold testing data"
test_urls = list()
for test_topic in test_topics:
    test_urls.append("http://www.debate.org" + test_topic)

test_output_comments = list()
test_output_sentiments = list()
test_output_num_comments = 0
for test_url in test_urls:
    print("...")
    test_sock = urllib.urlopen(test_url)
    test_html_src = test_sock.read()
    test_sock.close()

    test_all_comments = re.findall(r'Comment:<\/strong>(.*)', test_html_src)
    test_num_comments = len(test_all_comments)
    test_output_num_comments += test_num_comments
    for i in xrange(0, test_num_comments):
        test_output_comments.append(test_all_comments[i])
        if i <= test_num_comments/2:
            test_output_sentiments.append(1)
        else:
            test_output_sentiments.append(0)

#remove \r (bugging up our output csv)
l = 0
for test_output_comment in test_output_comments:
    test_output_comments[l] = test_output_comment.strip("\r")
    l += 1

outdict = dict()
outdict.update({"comment":test_output_comments})
outdict.update({"sentiment":test_output_sentiments})

test_output = pd.DataFrame(data=outdict, index=range(test_output_num_comments))
test_output.to_csv("labeledDebateTestData_Gold.csv")
{% endhighlight %}


To perform sentiment analysis: `debateSA.py`.
{% highlight python %}
import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier

if __name__ == '__main__':
    #--read data--#
    train = pd.read_csv("labeledDebateTrainData.csv")

    num_reviews = train["comment"].size

    print("Creating the bag of words...\n")
    #init bag of words tool, analyze words, max vocabulary of 5000 words
    vectorizer = CountVectorizer(analyzer="word", tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)

    #fit_transform, input is a list of str
    # 1. fit model (learn vocab)
    # 2. transform train data into feature vector
    train_data_features = vectorizer.fit_transform(train["comment"])

    #arrs are easy to work with
    train_data_features = train_data_features.toarray()

    #vocab = vectorizer.get_feature_names()

    print "Training the random forest..."

    #init w/ 100 trees
    forest = RandomForestClassifier(n_estimators=100)
    forest = forest.fit(train_data_features, train["sentiment"])

    #--DONE TRAINING--#
    #--     ...     --#
    #--Begin testing--#

    test = pd.read_csv("labeledDebateTestData.csv")
    #print test.shape

    #get bag of words for test set, convert to numpy arr
    test_data_features = vectorizer.transform(test["comment"])
    test_data_features = test_data_features.toarray()

    result = forest.predict(test_data_features)

    #cp results into dataframe
    output = pd.DataFrame(data = {"comment":test["comment"], "sentiment":result})
    output.to_csv( "Bag_of_Words_model_debateorg.csv")
{% endhighlight %}





### Results

* Achieve ~70% accuracy on predicting a user's take on an issue.

[Download Files](/assets/img/research/files.zip){: .btn}



<!-----
layout: post
title:  "A Post with a Video"
date:   2016-03-15
excerpt: "Custom written post descriptions are the way to go... if you're not lazy."
project: true
tag:
- sample
- post
- video
comments: true
----->

<!--<iframe width="560" height="315" src="//www.youtube.com/embed/SU3kYxJmWuQ" frameborder="0"> </iframe>

Video embeds are responsive and scale with the width of the main content block with the help of [FitVids](http://fitvidsjs.com/).

Not sure if this only effects Kramdown or if it's an issue with Markdown in general. But adding YouTube video embeds causes errors when building your Jekyll site. To fix add a space between the `<iframe>` tags and remove `allowfullscreen`. Example below:

{% highlight html %}
<iframe width="560" height="315" src="//www.youtube.com/embed/SU3kYxJmWuQ" frameborder="0"> </iframe>
{% endhighlight %}-->
